{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 환경설정 & 공통 상수 ===\n",
    "import requests, time, re, json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# 네이버가 빈 페이지/리다이렉트 주는 걸 막기 위한 헤더\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en;q=0.8\",\n",
    "    \"Referer\": \"https://www.naver.com/\"\n",
    "}\n",
    "\n",
    "# 프로젝트 경로(노트북 기준)\n",
    "RAW = Path(\"../data/raw\")\n",
    "PROC = Path(\"../data/processed\")\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✅ 환경 준비 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 기사 URL 패턴 & 유틸 ===\n",
    "ARTICLE_PATTERNS = [\n",
    "    re.compile(r\"https?://n\\.news\\.naver\\.com/article/\\d+/\\d+\"),\n",
    "    re.compile(r\"https?://n\\.news\\.naver\\.com/mnews/article/\\d+/\\d+\"),\n",
    "    re.compile(r\"https?://news\\.naver\\.com/main/read\\.naver.*[?&]oid=\\d+.*[?&]aid=\\d+\"),\n",
    "    re.compile(r\"https?://mnews\\.naver\\.com/article/\\d+/\\d+\"),\n",
    "]\n",
    "\n",
    "def is_article_url(href: str) -> bool:\n",
    "    if not href:\n",
    "        return False\n",
    "    return any(p.search(href) for p in ARTICLE_PATTERNS)\n",
    "\n",
    "def extract_oid_aid(url: str):\n",
    "    # n.news (PC/모바일)\n",
    "    m1 = re.search(r\"n\\.news\\.naver\\.com/(?:mnews/)?article/(\\d+)/(\\d+)\", url)\n",
    "    if m1:\n",
    "        return m1.group(1), m1.group(2)\n",
    "    # news.naver.com read.naver\n",
    "    m2_oid = re.search(r\"[?&]oid=(\\d+)\", url)\n",
    "    m2_aid = re.search(r\"[?&]aid=(\\d+)\", url)\n",
    "    if m2_oid and m2_aid:\n",
    "        return m2_oid.group(1), m2_aid.group(1)\n",
    "    return None, None\n",
    "\n",
    "def resolve_article_title(url, timeout=10):\n",
    "    \"\"\"기사 페이지에 직접 접속해 제목(og:title 우선)을 가져온다.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        og = soup.select_one('meta[property=\"og:title\"]')\n",
    "        if og and og.get(\"content\"):\n",
    "            return og[\"content\"].strip()\n",
    "        if soup.title and soup.title.string:\n",
    "            return soup.title.string.strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "print(\"✅ 유틸 로드\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 뉴스 링크 수집 (검색 결과 HTML → 기사 URL) ===\n",
    "def get_news_links_html(query, pages=15, sleep_sec=0.25, issue_keywords=None):\n",
    "    base = \"https://search.naver.com/search.naver\"\n",
    "    rows = []\n",
    "\n",
    "    for start_idx in range(1, pages*10, 10):  # 1, 11, 21, ...\n",
    "        params = {\"where\": \"news\", \"query\": query, \"start\": start_idx}\n",
    "        res = requests.get(base, headers=HEADERS, params=params, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # 대표 선택자 → 없으면 전체 a[href]에서 기사 패턴만\n",
    "        anchors = soup.select(\"a.title_link\")\n",
    "        if not anchors:\n",
    "            anchors = soup.select(\"a.news_tit\")\n",
    "        if not anchors:\n",
    "            anchors = [a for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "        kept = 0\n",
    "        for a in anchors:\n",
    "            href = a.get(\"href\", \"\")\n",
    "            if not is_article_url(href):\n",
    "                continue\n",
    "            title = a.get_text(strip=True) or \"네이버뉴스\"\n",
    "\n",
    "            # (선택) 제목 키워드 필터\n",
    "            if issue_keywords:\n",
    "                t = title.lower()\n",
    "                if not any(k.lower() in t for k in issue_keywords):\n",
    "                    continue\n",
    "\n",
    "            rows.append({\"title\": title, \"url\": href, \"query\": query})\n",
    "            kept += 1\n",
    "\n",
    "        print(f\"page {start_idx}: kept {kept}\")\n",
    "        if kept == 0 and start_idx == 1:\n",
    "            break  # 첫 페이지부터 0이면 중단\n",
    "\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "print(\"✅ 링크 수집 함수 준비\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#롬앤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 실행 파라미터 (브랜드별로 바꿔 쓰기) ===\n",
    "brand_code = \"romand\"  # 저장 파일명 등에 사용\n",
    "brand_query = \"롬앤 구순염\"   # 검색 쿼리(넓게)\n",
    "issue_keywords = None \n",
    "\n",
    "\n",
    "queries = [\"롬앤\", \"롬앤 틴트\", \"롬앤 논란\", \"롬앤 입술\", \"롬앤 알레르기\"]\n",
    "\n",
    "print(\"✅ 파라미터:\", brand_code, brand_query, issue_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 링크 수집 (넓게)\n",
    "html_df = get_news_links_html(brand_query, pages=15, issue_keywords=issue_keywords)\n",
    "print(\"수집 기사 수:\", len(html_df))\n",
    "display(html_df.head(3))\n",
    "\n",
    "# 2) 기사 페이지에서 실제 제목(og:title) 해석\n",
    "if len(html_df):\n",
    "    html_df[\"title_resolved\"] = html_df[\"url\"].apply(resolve_article_title)\n",
    "    # 너무 빠른 요청 방지\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# 3) 제목 기반 이슈 필터 (필요 시 조정)\n",
    "issue_kw = [\"구순염\",\"염증\",\"알레르기\",\"입술\",\"자극\",\"논란\",\"부작용\",\"사과\",\"보상\",\"기만\",\"해명\",\"문의\",\"CS\"]\n",
    "pat = \"|\".join(issue_kw)\n",
    "filtered = (html_df\n",
    "            .dropna(subset=[\"title_resolved\"])\n",
    "            .loc[html_df[\"title_resolved\"].str.contains(pat, case=False, na=False)]\n",
    "            .drop_duplicates(subset=[\"url\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "print(\"이슈 필터 후:\", len(filtered))\n",
    "display(filtered.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 댓글 API 수집 ===\n",
    "def get_naver_comments(oid, aid, max_pages=20, sleep_sec=0.2):\n",
    "    \"\"\"\n",
    "    기사 한 개의 댓글을 수집한다. (최대 max_pages, 페이지당 20개)\n",
    "    반환: 리스트[ {user, contents, regTime}, ... ]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        api = \"https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json\"\n",
    "        params = {\n",
    "            \"ticket\": \"news\",\n",
    "            \"templateId\": \"default_news\",\n",
    "            \"pool\": \"cbox5\",\n",
    "            \"lang\": \"ko\",\n",
    "            \"country\": \"KR\",\n",
    "            \"objectId\": f\"news{oid},{aid}\",\n",
    "            \"pageSize\": 20,\n",
    "            \"pageType\": 1,\n",
    "            \"page\": page,\n",
    "            \"sort\": \"FAVORITE\"\n",
    "        }\n",
    "        try:\n",
    "            res = requests.get(api, headers=HEADERS, params=params, timeout=10)\n",
    "        except Exception as e:\n",
    "            print(f\"  comments req err: {e}\")\n",
    "            break\n",
    "\n",
    "        txt = res.text.strip()\n",
    "        m = re.search(r\"\\((\\{.*\\})\\)$\", txt)  # JSONP → JSON\n",
    "        if not m:\n",
    "            break\n",
    "        data = json.loads(m.group(1))\n",
    "        lst = data.get(\"result\", {}).get(\"commentList\", [])\n",
    "        if not lst:\n",
    "            break\n",
    "\n",
    "        for c in lst:\n",
    "            out.append({\n",
    "                \"user\": c.get(\"userNameMasked\"),\n",
    "                \"contents\": c.get(\"contents\"),\n",
    "                \"regTime\": c.get(\"regTime\")\n",
    "            })\n",
    "        time.sleep(sleep_sec)\n",
    "    return out\n",
    "\n",
    "def collect_comments_for_links(df_links, brand_code, max_articles=15, max_comment_pages=10):\n",
    "    \"\"\"\n",
    "    링크 DF → oid/aid 추출 → 댓글 수집 → CSV 저장\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    target = df_links.head(max_articles)\n",
    "    for i, r in target.iterrows():\n",
    "        oid, aid = extract_oid_aid(r[\"url\"])\n",
    "        if not oid:\n",
    "            continue\n",
    "        print(f\"[{i+1}/{len(target)}] oid={oid}, aid={aid}\")\n",
    "        cmts = get_naver_comments(oid, aid, max_pages=max_comment_pages)\n",
    "        for c in cmts:\n",
    "            rows.append({\n",
    "                \"brand\": brand_code,\n",
    "                \"title\": r.get(\"title_resolved\") or r.get(\"title\"),\n",
    "                \"url\": r[\"url\"],\n",
    "                \"contents\": c[\"contents\"],\n",
    "                \"regTime\": c[\"regTime\"]\n",
    "            })\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out_path = RAW / f\"comments_{brand_code}.csv\"\n",
    "    out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ 저장:\", out_path, \"rows:\", len(out))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 대상 링크 선택: 필터 결과가 있으면 그걸, 없으면 원본 중 상위 N개\n",
    "links_for_comments = filtered if len(filtered) else html_df\n",
    "\n",
    "if len(links_for_comments) == 0:\n",
    "    print(\"❗ 기사 링크가 없습니다. query/필터를 완화하거나 pages를 늘려 다시 시도하세요.\")\n",
    "else:\n",
    "    comments_df = collect_comments_for_links(links_for_comments, brand_code,\n",
    "                                             max_articles=10, max_comment_pages=10)\n",
    "    display(comments_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#클리오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 실행 파라미터 (브랜드별로 바꿔 쓰기) ===\n",
    "brand_code = \"clio\"  # 저장 파일명 등에 사용\n",
    "brand_query = \"클리오 후기\"   # 검색 쿼리(넓게)\n",
    "issue_keywords = None \n",
    "\n",
    "\n",
    "queries = [\"클리오\", \"클리오 쿠션\", \"클리오 논란\", \"클리오 입술\", \"클리오 알레르기\"]\n",
    "\n",
    "print(\"✅ 파라미터:\", brand_code, brand_query, issue_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 링크 수집 (넓게)\n",
    "html_df = get_news_links_html(brand_query, pages=15, issue_keywords=issue_keywords)\n",
    "print(\"수집 기사 수:\", len(html_df))\n",
    "display(html_df.head(3))\n",
    "\n",
    "# 2) 기사 페이지에서 실제 제목(og:title) 해석\n",
    "if len(html_df):\n",
    "    html_df[\"title_resolved\"] = html_df[\"url\"].apply(resolve_article_title)\n",
    "    # 너무 빠른 요청 방지\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# 3) 제목 기반 이슈 필터 (필요 시 조정)\n",
    "issue_kw = [\"클리오\",\"각질 부각\",\"피부병\",\"피부\",\"자극\",\"논란\",\"부작용\",\"보상\",\"기만\",\"해명\",\"문의\",\"CS\"]\n",
    "pat = \"|\".join(issue_kw)\n",
    "filtered = (html_df\n",
    "            .dropna(subset=[\"title_resolved\"])\n",
    "            .loc[html_df[\"title_resolved\"].str.contains(pat, case=False, na=False)]\n",
    "            .drop_duplicates(subset=[\"url\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "print(\"이슈 필터 후:\", len(filtered))\n",
    "display(filtered.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 댓글 API 수집 ===\n",
    "def get_naver_comments(oid, aid, max_pages=20, sleep_sec=0.2):\n",
    "    \"\"\"\n",
    "    기사 한 개의 댓글을 수집한다. (최대 max_pages, 페이지당 20개)\n",
    "    반환: 리스트[ {user, contents, regTime}, ... ]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        api = \"https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json\"\n",
    "        params = {\n",
    "            \"ticket\": \"news\",\n",
    "            \"templateId\": \"default_news\",\n",
    "            \"pool\": \"cbox5\",\n",
    "            \"lang\": \"ko\",\n",
    "            \"country\": \"KR\",\n",
    "            \"objectId\": f\"news{oid},{aid}\",\n",
    "            \"pageSize\": 20,\n",
    "            \"pageType\": 1,\n",
    "            \"page\": page,\n",
    "            \"sort\": \"FAVORITE\"\n",
    "        }\n",
    "        try:\n",
    "            res = requests.get(api, headers=HEADERS, params=params, timeout=10)\n",
    "        except Exception as e:\n",
    "            print(f\"  comments req err: {e}\")\n",
    "            break\n",
    "\n",
    "        txt = res.text.strip()\n",
    "        m = re.search(r\"\\((\\{.*\\})\\)$\", txt)  # JSONP → JSON\n",
    "        if not m:\n",
    "            break\n",
    "        data = json.loads(m.group(1))\n",
    "        lst = data.get(\"result\", {}).get(\"commentList\", [])\n",
    "        if not lst:\n",
    "            break\n",
    "\n",
    "        for c in lst:\n",
    "            out.append({\n",
    "                \"user\": c.get(\"userNameMasked\"),\n",
    "                \"contents\": c.get(\"contents\"),\n",
    "                \"regTime\": c.get(\"regTime\")\n",
    "            })\n",
    "        time.sleep(sleep_sec)\n",
    "    return out\n",
    "\n",
    "def collect_comments_for_links(df_links, brand_code, max_articles=15, max_comment_pages=10):\n",
    "    \"\"\"\n",
    "    링크 DF → oid/aid 추출 → 댓글 수집 → CSV 저장\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    target = df_links.head(max_articles)\n",
    "    for i, r in target.iterrows():\n",
    "        oid, aid = extract_oid_aid(r[\"url\"])\n",
    "        if not oid:\n",
    "            continue\n",
    "        print(f\"[{i+1}/{len(target)}] oid={oid}, aid={aid}\")\n",
    "        cmts = get_naver_comments(oid, aid, max_pages=max_comment_pages)\n",
    "        for c in cmts:\n",
    "            rows.append({\n",
    "                \"brand\": brand_code,\n",
    "                \"title\": r.get(\"title_resolved\") or r.get(\"title\"),\n",
    "                \"url\": r[\"url\"],\n",
    "                \"contents\": c[\"contents\"],\n",
    "                \"regTime\": c[\"regTime\"]\n",
    "            })\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out_path = RAW / f\"comments_{brand_code}.csv\"\n",
    "    out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ 저장:\", out_path, \"rows:\", len(out))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 대상 링크 선택: 필터 결과가 있으면 그걸, 없으면 원본 중 상위 N개\n",
    "links_for_comments = filtered if len(filtered) else html_df\n",
    "\n",
    "if len(links_for_comments) == 0:\n",
    "    print(\"❗ 기사 링크가 없습니다. query/필터를 완화하거나 pages를 늘려 다시 시도하세요.\")\n",
    "else:\n",
    "    comments_df = collect_comments_for_links(links_for_comments, brand_code,\n",
    "                                             max_articles=10, max_comment_pages=10)\n",
    "    display(comments_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#에뛰드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 실행 파라미터 (브랜드별로 바꿔 쓰기) ===\n",
    "brand_code = \"etude\"  # 저장 파일명 등에 사용\n",
    "brand_query = \"에뛰드 논란\"   # 검색 쿼리(넓게)\n",
    "issue_keywords = None \n",
    "\n",
    "\n",
    "queries = [\"에뛰드\", \"에뛰드 판매중지\", \"중금속\", \"구순염\", \"알레르기\"]\n",
    "\n",
    "print(\"✅ 파라미터:\", brand_code, brand_query, issue_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 링크 수집 (넓게)\n",
    "html_df = get_news_links_html(brand_query, pages=15, issue_keywords=issue_keywords)\n",
    "print(\"수집 기사 수:\", len(html_df))\n",
    "display(html_df.head(3))\n",
    "\n",
    "# 2) 기사 페이지에서 실제 제목(og:title) 해석\n",
    "if len(html_df):\n",
    "    html_df[\"title_resolved\"] = html_df[\"url\"].apply(resolve_article_title)\n",
    "    # 너무 빠른 요청 방지\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# 3) 제목 기반 이슈 필터 (필요 시 조정)\n",
    "issue_kw = [\"에뛰드\",\"중금속\",\"피부병\",\"논란\",\"부작용\",\"보상\",\"기만\",\"해명\",\"자극\",\"성분\"]\n",
    "pat = \"|\".join(issue_kw)\n",
    "filtered = (html_df\n",
    "            .dropna(subset=[\"title_resolved\"])\n",
    "            .loc[html_df[\"title_resolved\"].str.contains(pat, case=False, na=False)]\n",
    "            .drop_duplicates(subset=[\"url\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "print(\"이슈 필터 후:\", len(filtered))\n",
    "display(filtered.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 댓글 API 수집 ===\n",
    "def get_naver_comments(oid, aid, max_pages=20, sleep_sec=0.2):\n",
    "    \"\"\"\n",
    "    기사 한 개의 댓글을 수집한다. (최대 max_pages, 페이지당 20개)\n",
    "    반환: 리스트[ {user, contents, regTime}, ... ]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        api = \"https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json\"\n",
    "        params = {\n",
    "            \"ticket\": \"news\",\n",
    "            \"templateId\": \"default_news\",\n",
    "            \"pool\": \"cbox5\",\n",
    "            \"lang\": \"ko\",\n",
    "            \"country\": \"KR\",\n",
    "            \"objectId\": f\"news{oid},{aid}\",\n",
    "            \"pageSize\": 20,\n",
    "            \"pageType\": 1,\n",
    "            \"page\": page,\n",
    "            \"sort\": \"FAVORITE\"\n",
    "        }\n",
    "        try:\n",
    "            res = requests.get(api, headers=HEADERS, params=params, timeout=10)\n",
    "        except Exception as e:\n",
    "            print(f\"  comments req err: {e}\")\n",
    "            break\n",
    "\n",
    "        txt = res.text.strip()\n",
    "        m = re.search(r\"\\((\\{.*\\})\\)$\", txt)  # JSONP → JSON\n",
    "        if not m:\n",
    "            break\n",
    "        data = json.loads(m.group(1))\n",
    "        lst = data.get(\"result\", {}).get(\"commentList\", [])\n",
    "        if not lst:\n",
    "            break\n",
    "\n",
    "        for c in lst:\n",
    "            out.append({\n",
    "                \"user\": c.get(\"userNameMasked\"),\n",
    "                \"contents\": c.get(\"contents\"),\n",
    "                \"regTime\": c.get(\"regTime\")\n",
    "            })\n",
    "        time.sleep(sleep_sec)\n",
    "    return out\n",
    "\n",
    "def collect_comments_for_links(df_links, brand_code, max_articles=15, max_comment_pages=10):\n",
    "    \"\"\"\n",
    "    링크 DF → oid/aid 추출 → 댓글 수집 → CSV 저장\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    target = df_links.head(max_articles)\n",
    "    for i, r in target.iterrows():\n",
    "        oid, aid = extract_oid_aid(r[\"url\"])\n",
    "        if not oid:\n",
    "            continue\n",
    "        print(f\"[{i+1}/{len(target)}] oid={oid}, aid={aid}\")\n",
    "        cmts = get_naver_comments(oid, aid, max_pages=max_comment_pages)\n",
    "        for c in cmts:\n",
    "            rows.append({\n",
    "                \"brand\": brand_code,\n",
    "                \"title\": r.get(\"title_resolved\") or r.get(\"title\"),\n",
    "                \"url\": r[\"url\"],\n",
    "                \"contents\": c[\"contents\"],\n",
    "                \"regTime\": c[\"regTime\"]\n",
    "            })\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out_path = RAW / f\"comments_{brand_code}.csv\"\n",
    "    out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ 저장:\", out_path, \"rows:\", len(out))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 대상 링크 선택: 필터 결과가 있으면 그걸, 없으면 원본 중 상위 N개\n",
    "links_for_comments = filtered if len(filtered) else html_df\n",
    "\n",
    "if len(links_for_comments) == 0:\n",
    "    print(\"❗ 기사 링크가 없습니다. query/필터를 완화하거나 pages를 늘려 다시 시도하세요.\")\n",
    "else:\n",
    "    comments_df = collect_comments_for_links(links_for_comments, brand_code,\n",
    "                                             max_articles=10, max_comment_pages=10)\n",
    "    display(comments_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이니스프리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 실행 파라미터 (브랜드별로 바꿔 쓰기) ===\n",
    "brand_code = \"innisfree\"  # 저장 파일명 등에 사용\n",
    "brand_query = \"이니스프리 논란\"   # 검색 쿼리(넓게)\n",
    "issue_keywords = None \n",
    "\n",
    "\n",
    "queries = [\"이니스프리\", \"종이공병\", \"피부병\", \"판매중지\", \"알레르기\"]\n",
    "\n",
    "print(\"✅ 파라미터:\", brand_code, brand_query, issue_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 링크 수집 (넓게)\n",
    "html_df = get_news_links_html(brand_query, pages=15, issue_keywords=issue_keywords)\n",
    "print(\"수집 기사 수:\", len(html_df))\n",
    "display(html_df.head(3))\n",
    "\n",
    "# 2) 기사 페이지에서 실제 제목(og:title) 해석\n",
    "if len(html_df):\n",
    "    html_df[\"title_resolved\"] = html_df[\"url\"].apply(resolve_article_title)\n",
    "    # 너무 빠른 요청 방지\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# 3) 제목 기반 이슈 필터 (필요 시 조정)\n",
    "issue_kw = [\"이니스프리\",\"종이병\",\"재활용\",\"논란\",\"부작용\",\"보상\",\"기만\",\"해명\",\"자극\",\"성분\"]\n",
    "pat = \"|\".join(issue_kw)\n",
    "filtered = (html_df\n",
    "            .dropna(subset=[\"title_resolved\"])\n",
    "            .loc[html_df[\"title_resolved\"].str.contains(pat, case=False, na=False)]\n",
    "            .drop_duplicates(subset=[\"url\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "print(\"이슈 필터 후:\", len(filtered))\n",
    "display(filtered.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 댓글 API 수집 ===\n",
    "def get_naver_comments(oid, aid, max_pages=20, sleep_sec=0.2):\n",
    "    \"\"\"\n",
    "    기사 한 개의 댓글을 수집한다. (최대 max_pages, 페이지당 20개)\n",
    "    반환: 리스트[ {user, contents, regTime}, ... ]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        api = \"https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json\"\n",
    "        params = {\n",
    "            \"ticket\": \"news\",\n",
    "            \"templateId\": \"default_news\",\n",
    "            \"pool\": \"cbox5\",\n",
    "            \"lang\": \"ko\",\n",
    "            \"country\": \"KR\",\n",
    "            \"objectId\": f\"news{oid},{aid}\",\n",
    "            \"pageSize\": 20,\n",
    "            \"pageType\": 1,\n",
    "            \"page\": page,\n",
    "            \"sort\": \"FAVORITE\"\n",
    "        }\n",
    "        try:\n",
    "            res = requests.get(api, headers=HEADERS, params=params, timeout=10)\n",
    "        except Exception as e:\n",
    "            print(f\"  comments req err: {e}\")\n",
    "            break\n",
    "\n",
    "        txt = res.text.strip()\n",
    "        m = re.search(r\"\\((\\{.*\\})\\)$\", txt)  # JSONP → JSON\n",
    "        if not m:\n",
    "            break\n",
    "        data = json.loads(m.group(1))\n",
    "        lst = data.get(\"result\", {}).get(\"commentList\", [])\n",
    "        if not lst:\n",
    "            break\n",
    "\n",
    "        for c in lst:\n",
    "            out.append({\n",
    "                \"user\": c.get(\"userNameMasked\"),\n",
    "                \"contents\": c.get(\"contents\"),\n",
    "                \"regTime\": c.get(\"regTime\")\n",
    "            })\n",
    "        time.sleep(sleep_sec)\n",
    "    return out\n",
    "\n",
    "def collect_comments_for_links(df_links, brand_code, max_articles=15, max_comment_pages=10):\n",
    "    \"\"\"\n",
    "    링크 DF → oid/aid 추출 → 댓글 수집 → CSV 저장\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    target = df_links.head(max_articles)\n",
    "    for i, r in target.iterrows():\n",
    "        oid, aid = extract_oid_aid(r[\"url\"])\n",
    "        if not oid:\n",
    "            continue\n",
    "        print(f\"[{i+1}/{len(target)}] oid={oid}, aid={aid}\")\n",
    "        cmts = get_naver_comments(oid, aid, max_pages=max_comment_pages)\n",
    "        for c in cmts:\n",
    "            rows.append({\n",
    "                \"brand\": brand_code,\n",
    "                \"title\": r.get(\"title_resolved\") or r.get(\"title\"),\n",
    "                \"url\": r[\"url\"],\n",
    "                \"contents\": c[\"contents\"],\n",
    "                \"regTime\": c[\"regTime\"]\n",
    "            })\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out_path = RAW / f\"comments_{brand_code}.csv\"\n",
    "    out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ 저장:\", out_path, \"rows:\", len(out))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 대상 링크 선택: 필터 결과가 있으면 그걸, 없으면 원본 중 상위 N개\n",
    "links_for_comments = filtered if len(filtered) else html_df\n",
    "\n",
    "if len(links_for_comments) == 0:\n",
    "    print(\"❗ 기사 링크가 없습니다. query/필터를 완화하거나 pages를 늘려 다시 시도하세요.\")\n",
    "else:\n",
    "    comments_df = collect_comments_for_links(links_for_comments, brand_code,\n",
    "                                             max_articles=10, max_comment_pages=10)\n",
    "    display(comments_df.head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
